{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a45bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required pkg\n",
    "import Pkg;\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"Interpolations\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"XLSX\")\n",
    "using Images, Statistics, LinearAlgebra, Plots, Interpolations, Random, DataFrames, XLSX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39399053",
   "metadata": {},
   "source": [
    "### [Auxiliary functions] to get: training_images, testing_images\n",
    "#### Each with a size = M * N^2\n",
    "\n",
    "M = Number of images (min_images_per_folder)\n",
    "\n",
    "N^2 = A vector of the original image_size*image_size image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get useful folders\n",
    "\n",
    "# Return a Vector{String} with all the folders with {min_images_per_folder} inside.\n",
    "function get_useful_folders(min_images)\n",
    "    ### Getting images\n",
    "    useful_folders = Vector{String}()\n",
    "    \n",
    "    itr = walkdir(base_dir)    \n",
    "    (root, dirs, files) = first(itr)\n",
    "    for d in dirs\n",
    "        images_on_folder = readdir(base_dir * d)\n",
    "        if size(images_on_folder)[1] >= min_images\n",
    "            push!(useful_folders, base_dir * d * \"/\")\n",
    "        end\n",
    "    end\n",
    "    useful_folders\n",
    "end\n",
    "\n",
    "# After filter by get_useful_folders(), get random {n_folders}\n",
    "function get_random_n_folders(min_images_per_folder)\n",
    "    # Get all the useful folders\n",
    "    useful_folders = get_useful_folders(min_images_per_folder)\n",
    "    # Get random folders from {useful_folders}\n",
    "    rand(useful_folders, n_folders)\n",
    "end\n",
    "\n",
    "# Get all images from {n_folders} random folders, each case for {training_images} and {testing_images}\n",
    "# Return a Matrix, size = M * N^2\n",
    "#     M = Number of images (min_images_per_folder)\n",
    "#     N^2 = A row vector of the original N*N image (now: 1*N^2)\n",
    "\n",
    "function get_images(float_type, working_with=nothing)\n",
    "    \n",
    "    # Pick {n_folders} to work with if {working_with} not provided\n",
    "    if working_with === nothing\n",
    "       working_with = get_random_n_folders(min_images_per_folder)\n",
    "    end    \n",
    "        \n",
    "    training_images = zeros(float_type, size(working_with, 1)*n_training_images,  image_size*image_size)\n",
    "    testing_images = zeros(float_type, size(working_with, 1)*n_testing_images,  image_size*image_size)\n",
    "    \n",
    "    # For every folder, get pictures\n",
    "    for (i, folder) in enumerate(working_with)\n",
    "        cdir = readdir(folder)        \n",
    "        \n",
    "        # Google Drive create a \"desktop.ini\", then I have to \"findfirst\" and delete element\n",
    "        splice!(cdir, findfirst(contains(\"desktop.ini\"), cdir))\n",
    "            \n",
    "        training_count = 1\n",
    "        testing_count = 1\n",
    "\n",
    "        for image_relative_path in cdir \n",
    "            \n",
    "            row_im = vec(float_type.(Gray.( imresize(load(folder * image_relative_path), image_size, image_size) )))' # 1 x N*N\n",
    "            #display(Gray.(load(folder * image_relative_path)))\n",
    "            \n",
    "            if training_count <= n_training_images\n",
    "                training_images[ (n_training_images*(i-1) + training_count),:] = row_im\n",
    "                training_count += 1\n",
    "            elseif testing_count <= n_testing_images\n",
    "                testing_images[ (n_testing_images*(i-1) + testing_count),:] = row_im\n",
    "                testing_count += 1\n",
    "            else\n",
    "                break  # Stop here if both training and testing images are collected\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    (training_images, testing_images)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function show_training_images(images)\n",
    "    for i in 1:n_folders\n",
    "        row = vcat([ Gray.(reshape(images[j,:], image_size, image_size)) for j in (n_training_images*(i-1))+1:((n_training_images*(i-1))+n_training_images)])\n",
    "        display(row)\n",
    "    end    \n",
    "end\n",
    "\n",
    "function show_testing_images(images)\n",
    "    for i in 1:n_folders\n",
    "        row = vcat([ Gray.(reshape(images[j,:], image_size, image_size)) for j in (n_testing_images*(i-1))+1:((n_testing_images*(i-1))+n_testing_images)])\n",
    "        display(row)\n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254de3e",
   "metadata": {},
   "source": [
    "# Getting the basics\n",
    "\n",
    "### μ, A, C, eigenvalues, eigenvectors\n",
    "\n",
    "#### n_folders\n",
    "Number of folders to use, randomly selected. **Between 1 and _size(get_useful_folders(n_folders), 1)_**\n",
    "\n",
    "#### n_training_images\n",
    "Number of training images to pick from every folder\n",
    "\n",
    "#### n_testing_images\n",
    "Same but for testing\n",
    "\n",
    "#### min_images_per_folder\n",
    "Before, it will filter the folders on the directory by {min_images_per_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./lfw/  = Color, not aligned\n",
    "# ./lfw2/ = Black and white, already aligned (eyes)\n",
    "base_dir = \"./lfw/\"\n",
    "image_size = 64\n",
    "\n",
    "n_folders = 10\n",
    "n_training_images = 12\n",
    "n_testing_images = 6\n",
    "\n",
    "min_images_per_folder = n_training_images + n_testing_images + 1; # +1 because google drive creating desktop.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07192e5",
   "metadata": {},
   "source": [
    "#### (training_images, testing_images), μ, A, eigenvalues, eigenvectors, elapsed_time, mem_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function setup_basics(float_type, folders=nothing)\n",
    "    (training_images, testing_images) = get_images(float_type, folders)\n",
    "    \n",
    "    # Mean\n",
    "    μ = mean(training_images, dims = 1)\n",
    "\n",
    "    # Centered matrix\n",
    "    A = training_images.-μ\n",
    "\n",
    "    # Covariance (N2xN2) and eigen\n",
    "    run_block = @timed begin\n",
    "        (eigenvalues, eigenvectors) = eigen(cov(A), sortby = x -> -x)\n",
    "    end\n",
    "    \n",
    "    (eigenvalues, eigenvectors) = run_block.value # Again\n",
    "    elapsed_time = run_block.time\n",
    "    mem_used = run_block.bytes / 1024^3 # Bytes to Gigabytes\n",
    "    \n",
    "    (training_images, testing_images), μ, A, eigenvalues, eigenvectors, elapsed_time, mem_used\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465428e",
   "metadata": {},
   "source": [
    "## Set folders to use, or random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a04b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folders to use, or random\n",
    "\n",
    "# Get random folders \n",
    "#random_n_folders = get_random_n_folders(min_images_per_folder)\n",
    "\n",
    "# Fixed\n",
    "random_n_folders = [\n",
    " \"./lfw/Guillermo_Coria/\",\n",
    " \"./lfw/Atal_Bihari_Vajpayee/\",\n",
    " \"./lfw/Kofi_Annan/\",\n",
    " \"./lfw/Nicole_Kidman/\",\n",
    " \"./lfw/Kofi_Annan/\",\n",
    " \"./lfw/Pervez_Musharraf/\",\n",
    " \"./lfw/Richard_Myers/\",\n",
    " \"./lfw/Tom_Daschle/\",\n",
    " \"./lfw/Amelie_Mauresmo/\",\n",
    " \"./lfw/Atal_Bihari_Vajpayee/\"\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3d772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8324fa6",
   "metadata": {},
   "source": [
    "# Reconstruction\n",
    "## Measuring _data_ differences for: Float 16, 32, 64, for every k on every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = [Float16, Float32, Float64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532e2c2",
   "metadata": {},
   "source": [
    "#### Setup (this block is repeated several times below for simplicity, but it's the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1aeeb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "\n",
    "n_folders = 16\n",
    "n_training_images = 32\n",
    "n_testing_images = 4\n",
    "\n",
    "min_images_per_folder = n_training_images + n_testing_images + 1; # +1 because google drive creating desktop.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86978c19",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `get_useful_folders` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `get_useful_folders` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[2]:2"
     ]
    }
   ],
   "source": [
    "# Check if there are as many folder after the filter using: min_images_per_folder\n",
    "println(\"We want $n_folders folders with $(min_images_per_folder-1) min_images_per_folder, we have: $(size(get_useful_folders(min_images_per_folder), 1))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5601538",
   "metadata": {},
   "source": [
    "#### [Auxiliary functions]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d39f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Returns a VECTOR with the norm error between original and reconstructed using k vectors.\n",
    "#\n",
    "#           |  k=1    |    k=2   |   ...   |  k = index_X_percent |\n",
    "#-----------------------------------------------------------------|\n",
    "#   Image   |  error  |   error  |  ...    |        error         |\n",
    "\n",
    "function reconstruct_for_k_eigenfaces(U, A, μ, training_images, display_images)\n",
    "    # Projections\n",
    "    P = transpose(U) * transpose(A)\n",
    "\n",
    "    # Reconstruction using k eigenvectors\n",
    "    reconstructed_images = (P' * U').+μ\n",
    "    \n",
    "    if display_images == true\n",
    "        display(\n",
    "            [\n",
    "                Gray.(reshape(training_images[2,:], image_size, image_size)), # Original\n",
    "                Gray.(reshape(reconstructed_images[2,:], image_size, image_size))  # Using k eigenfaces\n",
    "            ]\n",
    "        )\n",
    "    end\n",
    "    return norm.(eachrow(training_images - reconstructed_images), 2) # row_norms\n",
    "end\n",
    "\n",
    "\n",
    "# For every precision creates a table with the different reconstruction errors for every k on every image.\n",
    "# Example:\n",
    "\n",
    "#           |  k=1    |    k=2   |   ...   |  k = index_X_percent |\n",
    "#-----------------------------------------------------------------|\n",
    "# Image_1   |  error  |   error  |  ...    |        error         |\n",
    "#-----------------------------------------------------------------|\n",
    "# Image_2   |  error  |   error  |  ...    |        error         |\n",
    "#-----------------------------------------------------------------|\n",
    "#   ...     |   ...   |    ...   |   ...   |        ...           |\n",
    "#-----------------------------------------------------------------|\n",
    "# Image_M   |  error  |   error  |  ...    |        error         |\n",
    "#-----------------------------------------------------------------|\n",
    "\n",
    "\n",
    "function reconstruc_all_for_X_percent_of_k(percent)\n",
    "    errors_by_precision = []\n",
    "    \n",
    "    # Random\n",
    "    training_set = get_random_n_folders(min_images_per_folder)\n",
    "    \n",
    "    for used_precision in precisions\n",
    "        \n",
    "        # Getting the basics\n",
    "        ((training_images, testing_images), μ, A, eigenvalues, eigenvectors, elapsed_time, mem_used) = setup_basics(used_precision, training_set);\n",
    "        \n",
    "        # Using the to XX% most significant of all eigenvalues\n",
    "        cumulative_eigenvalues = cumsum(eigenvalues./sum(eigenvalues))\n",
    "        index_X_percent = findfirst(cumulative_eigenvalues .>= (percent/100))\n",
    "        \n",
    "        errors_by_k = zeros(Float64, n_folders*n_training_images, index_X_percent)\n",
    "        \n",
    "        display_data = false\n",
    "        for k in 1:index_X_percent\n",
    "            \n",
    "            if k == index_X_percent\n",
    "                display_data = true\n",
    "            end\n",
    "            errors_by_k[:,k] = reconstruct_for_k_eigenfaces(eigenvectors[:, 1:k], A, μ, training_images, display_data)\n",
    "        end\n",
    "            \n",
    "        push!(errors_by_precision, errors_by_k)\n",
    "    end\n",
    "    errors_by_precision\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c918a7",
   "metadata": {},
   "source": [
    "#### Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1bc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_by_precision = reconstruc_all_for_X_percent_of_k(99);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2502f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = XLSX.writetable(\"temp_table.xlsx\", DataFrame(errors_by_precision[3], :auto),  overwrite=true, sheetname=\"Float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c87bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean( vcat(errors_by_precision[3][:,end], errors_by_precision[2][:,end], errors_by_precision[1][:,end]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Mean Float16: $(mean(errors_by_precision[1][:,end]))\")\n",
    "println(\"Mean Float32: $(mean(errors_by_precision[2][:,end]))\")\n",
    "println(\"Mean Float64: $(mean(errors_by_precision[3][:,end]))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_by_precision[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_by_precision[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a997f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_by_precision[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d612f5",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with eps 64\n",
    "epsilon = eps(Float16)\n",
    "\n",
    "# Absolite diff bigger than eps? (64 vs 16)\n",
    "differences = errors_by_precision[3] - errors_by_precision[1]\n",
    "result = abs.(differences) .> epsilon\n",
    "\n",
    "# Results\n",
    "println(\"Differences: \")\n",
    "println(differences)\n",
    "println(\"Results: \")\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d796af",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba3b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e509535",
   "metadata": {},
   "source": [
    "## Measuring _mem and time_ differences for: Float 16, 32, 64 on setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55ba77",
   "metadata": {},
   "source": [
    "#### Run block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a045bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_n_folders[end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee6aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Memory and time used to calculate covariance matrix and eigen for F16, 32, 64\n",
    "#\n",
    "mem_time_by_precision = []\n",
    "    \n",
    "#\n",
    "# Original values -> Aux\n",
    "aux_n_folders = n_folders\n",
    "aux_n_training_images = n_training_images\n",
    "aux_n_testing_images = n_testing_images\n",
    "aux_random_n_folders = random_n_folders\n",
    "\n",
    "# Different values to test:\n",
    "array_n_folders = [4, 8] # <-------- Modify this\n",
    "array_n_training_images = [8, 16] # <-------- Modify this\n",
    "n_testing_images = 4\n",
    "\n",
    "# Update {min_images_per_folder} using the bigger values\n",
    "min_images_per_folder = n_testing_images + array_n_training_images[end] + 1; # +1 because google drive creating desktop.ini\n",
    "\n",
    "# Use the new {min_images_per_folder} to get the folders\n",
    "training_set = get_random_n_folders(min_images_per_folder)\n",
    "\n",
    "for used_precision in precisions\n",
    "\n",
    "    data = zeros(Float64, size(array_n_folders,1)*size(array_n_training_images,1), 4) \n",
    "    index = 1\n",
    "    # Table data:\n",
    "\n",
    "    # n_folders | n_training_images | elapsed_time | mem_used\n",
    "    #    ..              ..                ..           ..\n",
    "    #    ..              ..                ..           ..\n",
    "\n",
    "    for aux1 in array_n_folders\n",
    "        n_folders = aux1\n",
    "\n",
    "        for aux2 in array_n_training_images\n",
    "            n_training_images = aux2\n",
    "\n",
    "            # Working here...\n",
    "            ((training_images, testing_images), μ, A, eigenvalues, eigenvectors, elapsed_time, mem_used) = setup_basics(used_precision, training_set[1:n_folders]);\n",
    "\n",
    "            # Save data:\n",
    "            data[index,:] = [n_folders, n_training_images, elapsed_time, mem_used]\n",
    "            index += 1            \n",
    "        end\n",
    "    end\n",
    "\n",
    "    push!(mem_time_by_precision, data)\n",
    "end\n",
    "\n",
    "# Aux -> Original values\n",
    "n_folders = aux_n_folders\n",
    "n_training_images = aux_n_training_images\n",
    "n_testing_images = aux_n_testing_images;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b724091",
   "metadata": {},
   "outputs": [],
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48941c",
   "metadata": {},
   "source": [
    "#### Show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encabezado de la tabla\n",
    "println(\"Float16:\")\n",
    "println(\"n_folders | n_training_images | elapsed_time | mem_used\")\n",
    "mem_time_by_precision[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Float32:\")\n",
    "mem_time_by_precision[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80885867",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Float64:\")\n",
    "mem_time_by_precision[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca7c63",
   "metadata": {},
   "source": [
    "## Plot eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9c71e",
   "metadata": {},
   "source": [
    "#### Setup data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_precision = Float32\n",
    "\n",
    "image_size = 128\n",
    "\n",
    "n_folders = 2\n",
    "n_training_images = 4\n",
    "n_testing_images = 4\n",
    "\n",
    "min_images_per_folder = n_training_images + n_testing_images + 1; # +1 because google drive creating desktop.ini\n",
    "\n",
    "((training_images, testing_images), μ, A, eigenvalues, eigenvectors, elapsed_time, mem_used) = setup_basics(used_precision);\n",
    "\n",
    "X_percent = 99\n",
    "X_percent = X_percent / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c8a04",
   "metadata": {},
   "source": [
    "#### Run block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_eigenvalues = cumsum(eigenvalues ./ sum(eigenvalues))\n",
    "index_X_percent = findfirst(cumulative_eigenvalues .>= X_percent)\n",
    "\n",
    "# To put X_percent in the middle\n",
    "x_range = 1:index_X_percent * 2\n",
    "\n",
    "# Plot 2 curves con el rango x limitado\n",
    "plot(\n",
    "    [eigenvalues[x_range] ./ sum(eigenvalues)], \n",
    "    label=\"eigenvalues\",\n",
    "    title=\"$(X_percent)% coverage at the $index_X_percent-th elem of $(size(eigenvalues, 1)) (~$(round(index_X_percent/(size(eigenvalues, 1)), digits=4))%)\",\n",
    "    legend=:outerbottom,\n",
    "    titlefontsize=12\n",
    ")\n",
    "\n",
    "# Plot cumulative eigenvalues con el rango x limitado\n",
    "plot!(cumulative_eigenvalues[x_range], label=\"cumulative eigenvalues\")\n",
    "\n",
    "# Add X% text\n",
    "annotate!([(index_X_percent, cumulative_eigenvalues[index_X_percent], text(\"$(X_percent)%\", 8))])\n",
    "\n",
    "# Add x line\n",
    "vline!([index_X_percent], line=:black, linewidth=0.5, label=nothing)\n",
    "\n",
    "# Add y line\n",
    "hline!([cumulative_eigenvalues[index_X_percent]], line=:black, linewidth=0.5, label=nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7699ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"2x8.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c7241",
   "metadata": {},
   "source": [
    "# Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44085a1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#show_training_images(training_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_testing_images(testing_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21418dc1",
   "metadata": {},
   "source": [
    "#### Setup data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed58e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "\n",
    "n_folders = 16\n",
    "n_training_images = 32\n",
    "n_testing_images = 8\n",
    "\n",
    "min_images_per_folder = n_training_images + n_testing_images + 1; # +1 because google drive creating desktop.ini\n",
    "\n",
    "# Selecting the top {X_percent}% of the eigenvalues\n",
    "X_percent = 99\n",
    "X_percent = X_percent / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82973aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are as many folder after the filter using: min_images_per_folder\n",
    "println(\"We want $n_folders folders with $(min_images_per_folder-1) min_images_per_folder, we have: $(size(get_useful_folders(min_images_per_folder), 1))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7b5e8",
   "metadata": {},
   "source": [
    "#### Aux functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b4834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use k components to test accuracy.\n",
    "\n",
    "function test_for_k_components(A, eigenfaces, random_n_folders, testing_images, display_data)\n",
    "    P = transpose(eigenfaces)*transpose(A);\n",
    "    \n",
    "    if display_data == true\n",
    "        println(\"Testing for: $(typeof(A)) and $(size(eigenfaces, 2)) eigenfaces\")\n",
    "    end\n",
    "\n",
    "    correct = 0.0\n",
    "    all_min_dif = [] # We collect all the min norm diff for every k\n",
    "    all_variance = [] # The variance between all images\n",
    "    \n",
    "    for image_test_index in 1:size(testing_images,1)\n",
    "        x_i = testing_images[image_test_index,:]' - μ\n",
    "        P_i = transpose(eigenfaces) * transpose(x_i);\n",
    "\n",
    "        dist=[]\n",
    "        for j in 1:size(P,2)\n",
    "            push!(dist, norm(P[:,j] - P_i))\n",
    "        end\n",
    "        pred = argmin(dist)\n",
    "            \n",
    "        push!(all_min_dif, dist[pred])\n",
    "        push!(all_variance, var(dist))\n",
    "\n",
    "        celeb_tested = random_n_folders[ div(image_test_index - 1, n_testing_images) + 1 ]            \n",
    "        celeb_predicted = random_n_folders[ div(pred - 1, n_training_images) + 1 ]\n",
    "        \n",
    "        if (celeb_tested == celeb_predicted)\n",
    "            correct+=1\n",
    "        end   \n",
    "                \n",
    "        if display_data == true\n",
    "            println(\"_________________________________\")\n",
    "            println(\"Current: $celeb_tested (#$image_test_index)\")\n",
    "            println(\"Pred:    $celeb_predicted (#$pred)\")\n",
    "\n",
    "            println(\"Correct: $(correct/image_test_index*100)% \")\n",
    "\n",
    "            IJulia.display([\n",
    "                Gray.(reshape(testing_images[image_test_index,:], image_size, image_size)),\n",
    "                Gray.(reshape(training_images[pred,:], image_size, image_size))\n",
    "            ])\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    accuracy = (correct/size(testing_images,1))*100\n",
    "    (accuracy, vec(all_min_dif), vec(all_variance))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619d1c3",
   "metadata": {},
   "source": [
    "#### Run block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688e04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The training set (USE THE SAME FOR EVERY PRECISION)\n",
    "training_set = get_random_n_folders(min_images_per_folder)\n",
    "\n",
    "# Firts run, to get eigenvalues and then -> index_X_percent\n",
    "used_precision = Float16\n",
    "((training_images, testing_images), μ, A, eigenvalues, eigenvectors, elapsed_time, mem_used) = setup_basics(used_precision, training_set);\n",
    "\n",
    "# Testing for the {X_percent}%\n",
    "cumulative_eigenvalues = cumsum(eigenvalues ./ sum(eigenvalues))\n",
    "index_X_percent = findfirst(cumulative_eigenvalues .>= X_percent)\n",
    "\n",
    "\n",
    "# Table format for {accuracy_by_precision}\n",
    "#\n",
    "#           |  Float16  |  Float32  |  Float64  |\n",
    "#-----------|-----------|-----------|-----------|\n",
    "#    k=1    | correct%  | correct%  | correct%  |\n",
    "#    k=2    | correct%  | correct%  | correct%  |\n",
    "#    ...    |   ...     |   ...     |   ...     |\n",
    "#   k = X%  | correct%  | correct%  | correct%  |\n",
    "\n",
    "\n",
    "# Table format for {recognition_dif_by_precision}\n",
    "#\n",
    "# We save the minimum difference (norm) for every test image, for each k (1 to 99% eigenface space)\n",
    "#\n",
    "#                |  k=1    |    k=2   |   ...   |  k = index_X_percent |\n",
    "#----------------------------------------------------------------------|\n",
    "# Test_image_1   |  error  |   error  |  ...    |        error         |\n",
    "#----------------------------------------------------------------------|\n",
    "# Test_image_2   |  error  |   error  |  ...    |        error         |\n",
    "#----------------------------------------------------------------------|\n",
    "#        ...     |   ...   |    ...   |   ...   |        ...           |\n",
    "#----------------------------------------------------------------------|\n",
    "# Test_image_M   |  error  |   error  |  ...    |        error         |\n",
    "#-----------------------------------------------------------------     |\n",
    "\n",
    "# Table format for {recognition_var_by_precision}\n",
    "#\n",
    "# The same, but for every test image on every k, we calculate de variance across all the matches, this is\n",
    "# projected training image vs a single projected testing image\n",
    "#\n",
    "#                |  k=1      |    k=2    |   ...    |  k = index_X_percent |\n",
    "#--------------------------------------------------------------------------|\n",
    "# Test_image_1   | total_var | total_var |   ...    |      total_var       |\n",
    "#--------------------------------------------------------------------------|\n",
    "# Test_image_2   | total_var | total_var |   ...    |      total_var       |\n",
    "#--------------------------------------------------------------------------|\n",
    "#        ...     |    ...    |    ...    |   ...    |        ...           |\n",
    "#--------------------------------------------------------------------------|\n",
    "# Test_image_M   | total_var | total_var |   ...    |      total_var       |\n",
    "#--------------------------------------------------------------------------|\n",
    "\n",
    "\n",
    "# The data:\n",
    "#\n",
    "accuracy_by_precision = zeros(Float64, index_X_percent, 3)\n",
    "recognition_dif_by_precision = []\n",
    "recognition_var_by_precision = []\n",
    "\n",
    "# TODO: Iterations?\n",
    "\n",
    "for (col, used_precision) in enumerate(precisions)\n",
    "    ((training_images, testing_images), μ, A, eigenvalues, eigenvectors, elapsed_time, mem_used) = setup_basics(used_precision, training_set);\n",
    "\n",
    "    recognition_diff = zeros(Float64, n_folders * n_testing_images, index_X_percent)\n",
    "    recognition_var = zeros(Float64, n_folders * n_testing_images, index_X_percent)\n",
    "        \n",
    "    for k in 1:index_X_percent\n",
    "\n",
    "        # accuracy = A single value\n",
    "        # diff     = A complete column\n",
    "        (accuracy, diff, var) = test_for_k_components(A, eigenvectors[:,1:k], training_set, testing_images, false)\n",
    "\n",
    "        accuracy_by_precision[k,col] = accuracy\n",
    "        recognition_diff[:,k] = diff\n",
    "        recognition_var[:,k] = var\n",
    "    end\n",
    "    \n",
    "    push!(recognition_dif_by_precision, recognition_diff)\n",
    "    push!(recognition_var_by_precision, recognition_var)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50728629",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_by_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc91cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    " plot(accuracy_by_precision[:,1], label = \"Float16\", title = \"Comparativa de porcentaje de aciertos\", xlabel = \"Número de eigenvectores usados\", ylabel = \"% de aciertos\")\n",
    "plot!(accuracy_by_precision[:,2], label = \"Float32\")\n",
    "plot!(accuracy_by_precision[:,3], label = \"Float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"comparativa.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbc1be",
   "metadata": {},
   "source": [
    "#### Minimum norm difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd60ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = XLSX.writetable(\"temp_table.xlsx\", DataFrame(recognition_dif_by_precision[1], :auto), overwrite=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aea309",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_dif_by_precision[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45045ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_dif_by_precision[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bba834",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_dif_by_precision[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fb81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5513878",
   "metadata": {},
   "outputs": [],
   "source": [
    "[((recognition_dif_by_precision[2][:, i] - recognition_dif_by_precision[1][:, i])) for i in 1:size(recognition_dif_by_precision[1], 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d296de",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps(Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73342d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_diff = hcat([Int16(k) for k in 1:size(recognition_dif_by_precision[1], 2)] ,[mean((recognition_dif_by_precision[2][:, i] - recognition_dif_by_precision[1][:, i]) .> eps(Float32))*100 for i in 1:size(recognition_dif_by_precision[1], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1dcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer columnas para x e y\n",
    "x = graph_diff[:, 1]\n",
    "y = graph_diff[:, 2]\n",
    "\n",
    "# Crear el gráfico de dispersión\n",
    "plot(x, y, marker=:circle, line=:auto, xlabel=\"Número de eigenvectores usados\", ylabel=\"% de diferencias que superaron eps\", titlefontsize=12, title = \"Comparativa Float32 y Float16 entre normas de diferencias\", legend=false)\n",
    "# Establecer el rango del eje y de 0 a 100%\n",
    "ylims!(0, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f07553",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"32vs16.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1e46f",
   "metadata": {},
   "source": [
    "#### Minimum var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_var_by_precision[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8104ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_var_by_precision[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_var_by_precision[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51065c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_var_by_precision[3]-recognition_var_by_precision[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_var = hcat([Int16(k) for k in 1:size(recognition_var_by_precision[1], 2)] ,[mean((recognition_var_by_precision[2][:, i] - recognition_var_by_precision[1][:, i]) .> eps(Float32))*100 for i in 1:size(recognition_dif_by_precision[1], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer columnas para x e y\n",
    "x = graph_var[:, 1]\n",
    "y = graph_var[:, 2]\n",
    "\n",
    "# Crear el gráfico de dispersión\n",
    "plot(x, y, marker=:circle, line=:auto, xlabel=\"Número de eigenvectores usados\", ylabel=\"% de diferencias que superaron eps\", titlefontsize=12, title = \"Comparativa Float32 y Float16 entre varianzas\", legend=false)\n",
    "# Establecer el rango del eje y de 0 a 100%\n",
    "ylims!(0, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"32vs16_var.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bc572",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60548a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with eps 64\n",
    "    epsilon = eps(Float32)\n",
    "\n",
    "# Abs diff bigger than eps?\n",
    "# [3] = 64\n",
    "# [2] = 32\n",
    "# [1] = 16\n",
    "result = abs.(recognition_dif_by_precision[3] - recognition_dif_by_precision[1]) .> epsilon\n",
    "\n",
    "# Results\n",
    "println(\"1 = Difference between values is > than epsilon $(typeof(epsilon))\")\n",
    "println(\"0 = No\\n\")\n",
    "println(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c8906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f9270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ef504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab9cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATA FRAMES ---> EXCEL\n",
    "\n",
    "# Define DF\n",
    "df = DataFrame(\n",
    "    used_precision = UInt8[],\n",
    "    image_size = UInt16[],\n",
    "    n_folders = UInt16[],\n",
    "    n_training_images = UInt8[],\n",
    "    n_testing_images = UInt8[],\n",
    "    eigen_elapsed_time = Float64[],\n",
    "    used_eigenfaces = UInt8[],\n",
    "    norm_error = Float64[],\n",
    "    acurracy = Float16[]\n",
    ")\n",
    "\n",
    "# Define a row\n",
    "row_dict = Dict{String, Any}()\n",
    "\n",
    "# Save data\n",
    "row_dict[\"used_precision\"] = p\n",
    "row_dict[\"image_size\"] = image_size\n",
    "row_dict[\"persons\"] = used_persons\n",
    "row_dict[\"images_db\"] = images_to_db\n",
    "row_dict[\"images_to_test\"] = images_to_test\n",
    "row_dict[\"covariance_calc_time\"] = covariance_elapsed_time\n",
    "row_dict[\"eigen_elapsed_time\"] = eigen_elapsed_time\n",
    "row_dict[\"used_eigenfaces\"] = n_components\n",
    "row_dict[\"norm_error\"] = error\n",
    "row_dict[\"acurracy\"] = acurracy\n",
    "\n",
    "# Push to DF\n",
    "push!(df, row_dict)\n",
    "\n",
    "# Write excel\n",
    "XLSX.writetable(\"eigenframe.xlsx\", df, overwrite=true, sheetname=\"Recognition\")\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35c8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
