{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Representation learning: variational autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning representations for handwritten digits\n",
    "\n",
    "In the previous practical session, we looked at developing a deep learning pipeline for classifying blood cells as healthy of parasitized. The model we trained was an example of a discriminative model $p_\\theta$, which uses samples from an unknown joint distribution $p(X,Y)$ to approximate $p(Y|X)$ as closely as possible. The optimal $p_{\\theta^*}$ can then be used to make predictions.\n",
    "\n",
    "In this session, we will look at generative models. These models use samples from an unknown data distribution $p_X$ to find the optimal $p_\\theta$ that approximates $P_X$. The model $p_{\\theta^*}$ can then be used to generate new samples from the approximated data distribution.\n",
    "\n",
    "In particular, we will use a Variational Autoencoder to approximate the data distribution of handwritten digits. For this we will use the default computer vision dataset: MNIST. \n",
    "\n",
    "This practical session is based on a [tutorial from the TensorFlow documentation](https://www.tensorflow.org/tutorials/generative/cvae)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn\n",
    "from matplotlib import cm\n",
    "import pandas\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def select_device(prefer_gpu=True):\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    if (len(gpus) > 0) and prefer_gpu:\n",
    "        return gpus[0]\n",
    "    else:\n",
    "        return [x.name for x in local_device_protos if x.device_type == 'CPU'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code sets the device to use to GPU if you have one available\n",
    "device = select_device(prefer_gpu=True)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `tensorflow-datasets` package to load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = tfds.builder('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.download_and_prepare() # can take up to 10 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some info about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the builder to extract a TensorFlow Dataset for efficient access to the images.\n",
    "We also split the dataset into training data for training the neural network parameters, and testing data for evaluating the network's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    train_ds, test_ds = (\n",
    "        builder.as_dataset(as_supervised=True, split=\"train\"), \n",
    "        builder.as_dataset(as_supervised=True, split=\"test\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10\n",
    "fig, axes = plt.subplots(1, n_images, figsize=(20, 5), dpi=100)\n",
    "for ax, (image, label) in zip(axes, train_ds.take(n_images)):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventhough we are only looking at black and white images of handwritten digits, $p_X$ is still quite a complex distribution to approximate. Therefore, we need to simplify the problem a little bit by thresholding the images.\n",
    "\n",
    "This is done in two steps:\n",
    "1. Normalize the pixel value range to [0,1].\n",
    "2. Threshold all pixels at 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the second preprocessing function\n",
    "def normalization(image, label):\n",
    "    return (image / 255, label)\n",
    "\n",
    "def threshold(image, label):\n",
    "    return (tf.where(image > 0.5, 1., 0.), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(normalization)\n",
    "test_ds = test_ds.map(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(threshold)\n",
    "test_ds = test_ds.map(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    n_images = 10\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(20, 5), dpi=100, sharex=True, sharey=True)\n",
    "    for ax, (image, label) in zip(axes, train_ds.take(n_images)):\n",
    "        ax.imshow(image[...,0], cmap=\"gray\")\n",
    "        ax.set_title(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    train_ds = train_ds.shuffle(builder.info.splits[\"train\"].num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now preprocessed and ready to handled by the classifier that we will define in the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a variational autoencdoer architecture for representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a computer vision task, we will use a **convolutional** variational autoencoder (CVAE). The CVAE has the same bottleneck structure as a standard variational autoencoder, but replaces the dense layers with convolutional and tranposed convolutional layers. The former type used in the encoder for down-sampling, and the latter in the decoder for upsampling. As with any autoencoder, the goal of the CVAE is to learn the *identity function*: learn to reconstruct the input image after \"squeezing\" it through a low-dimensional latent representation, also called the *bottleneck*.\n",
    "\n",
    "Additionally, we will use a **variational** autoencoder for this task. This means we will use variational inference to find the optimal $\\theta$ and $\\lambda$ to parameterize, respectively, the decoder $p_\\theta(x|z)$ and the encoder $q_\\lambda(z|x)$, with $z$ the latent representation.\n",
    "\n",
    "In order for us to be able to use $p_\\theta(x|z)$, we need to first be able to choose a meaningfull $z$. Therefore, we will impose a Gaussian prior over $q_\\lambda(z|x)$. We'll see how this is done when we define the loss\n",
    "\n",
    "A high-level overview of the CVAE architecture is shown below:\n",
    "![SegmentLocal](vae-gaussian.png \"architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reparameterization trick\n",
    "\n",
    "The reparameterization trick is a very important part of the VAE. It allows us to learn a distribution over the random variable $Z$ (= the latent representation). Without the reparameterization trick, backpropagation would be stuck at the random bottleneck node. We cannot backpropagate through a random variable.\n",
    "\n",
    "Therefore, we introduce a normally distributed random variable $\\epsilon$ and we let the encoder output the parameters of a multivariate normal distribution as a vector. A latent representation for an input $x$ can then be constructed as follows:\n",
    "1. Sample from $\\epsilon\\sim\\mathcal{N}(O, \\mathbb{1})$\n",
    "2. $z = \\mu + \\sigma \\odot \\epsilon$\n",
    "\n",
    "This $z$ can then be passed on to the decoder. The backpropagation can now flow through the parameter vector, that is output by the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**\n",
    "\n",
    "Implement the reparameterization trick in the `reparameterize` function below.\n",
    "\n",
    "**NOTE**:\n",
    "We use the log-normal distribution for numerical stability during training. This means:\n",
    "\n",
    "$$ \\sigma = \\sqrt{e^\\textrm{logvar}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width, image_height = 28,28\n",
    "\n",
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        subsampled_size = 7\n",
    "        input_shape = (image_width, image_height, 1)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = tf.keras.Sequential(\n",
    "          [\n",
    "              tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "              tf.keras.layers.Conv2D( # This layer will downsample the image to half its width and height\n",
    "                  filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "              tf.keras.layers.Conv2D( # This layer will downsample the image again to half its current width and height\n",
    "                  filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              # No activation\n",
    "              tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "          ]\n",
    "        )\n",
    "\n",
    "        self.generative_net = tf.keras.Sequential(\n",
    "            [\n",
    "              tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "              tf.keras.layers.Dense(units=subsampled_size*subsampled_size*32, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Reshape(target_shape=(subsampled_size, subsampled_size, 32)),\n",
    "              tf.keras.layers.Conv2DTranspose( # This layer will upsample the image to twice its current width and height\n",
    "                  filters=64,\n",
    "                  kernel_size=3,\n",
    "                  strides=(2, 2),\n",
    "                  padding=\"SAME\",\n",
    "                  activation='relu'),\n",
    "              tf.keras.layers.Conv2DTranspose( # This layer will upsample the image again to twice its current width and height\n",
    "                  filters=32,\n",
    "                  kernel_size=3,\n",
    "                  strides=(2, 2),\n",
    "                  padding=\"SAME\",\n",
    "                  activation='relu'),\n",
    "              # No activation\n",
    "              tf.keras.layers.Conv2DTranspose(\n",
    "                  filters=input_shape[-1], kernel_size=3, strides=(1, 1), padding=\"SAME\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        ...\n",
    "        return sample\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The $\\beta$-VAE loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the theory lecture on Representation learning, the end goal of training a variational auto-encoder is finding a model that approximates the data generating distribution $p_{data}$:\n",
    "\n",
    "$$\\theta*=\\textrm{arg}\\textrm{min}_{\\theta\\in\\mathcal{M}}d(p_{data},p_\\theta)$$\n",
    "\n",
    "with $d$ the Kullback-Leibler divergence.\n",
    "\n",
    "It can be shown that this minimization is equivalent to maximizing the log-likelihood $log{p_\\theta}$ over a discrete dataset sampled from $p_{data}$. However, computing $log{p_\\theta}$ is intractable due to the marginalization over $Z$, the latent random variable.\n",
    "\n",
    "Therefore, we approximate this problem by maximizing a (tractable) lower-bound on $log{p_\\theta}$, namely, the Evidence Lower BOund (ELBO):\n",
    "\n",
    "$$\\log p_\\theta(x) \\geq \\mathbb{E}_{z\\sim q_\\lambda(z|x)}[\\log{\\frac{p_\\theta(x|z)}{q_\\lambda(z|x)}}]$$\n",
    "\n",
    "In order to be able to sample meaningful latent representations, we impose a unit Gaussian prior on $Z$.\n",
    "\n",
    "This allows us to construct the VAE loss function:\n",
    "\n",
    "$$\\mathcal{L}(x; \\theta, \\lambda) = \\mathbb{E}_{z\\sim q_\\lambda(z|x)}[\\log{p_\\theta(x|z)}] - d_{KL}(q_\\lambda(z|x), p_\\theta(z))$$\n",
    "\n",
    "By weighting the $d_{KL}$-term, we obtain the $\\beta$-VAE loss function:\n",
    "\n",
    "$$\\mathcal{L}(x; \\theta, \\lambda, \\beta) = \\mathbb{E}_{z\\sim q_\\lambda(z|x)}[\\log{p_\\theta(x|z)}] - \\beta*d_{KL}(q_\\lambda(z|x), p_\\theta(z))$$\n",
    "\n",
    "Here, we approximate $\\mathcal{L}(x; \\theta, \\lambda, \\beta)$ using a single-sample Monte Carlo Estimate:\n",
    "\n",
    "$$\\log{p_\\theta(x|z)} + \\beta*(\\log{p_\\theta(z)} - \\log{q_\\lambda(z|x)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**\n",
    "\n",
    "Complete the missing parts in the function below to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(model, x_true, beta=1):\n",
    "    mu, logvar = ... # parameters for the multivariate normal posterior\n",
    "    z_sample = ... # a sample from the posterior\n",
    "    x_recons_logits = ... # the reconstruction of the sample\n",
    "    \n",
    "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x_true,\n",
    "                        logits=x_recons_logits)\n",
    "    \n",
    "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1, 2, 3])\n",
    "    \n",
    "    logpz = ... # density of the prior evaluated at z_sample\n",
    "    logqz_x = ...  # density of the latent posterior evaluated at z_sample\n",
    "    kl_divergence = logqz_x - logpz\n",
    "    \n",
    "    elbo = tf.math.reduce_mean(-beta * kl_divergence - neg_log_likelihood)\n",
    "    \n",
    "    return dict(\n",
    "        loss=-elbo, \n",
    "        reconstruction=-neg_log_likelihood, \n",
    "        kl=-kl_divergence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_apply_gradients(model, x, optimizer, beta=1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x, beta=beta)[\"loss\"]\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run learning procedure to train the network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some constants for the learning procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "train_len = builder.info.splits[\"train\"].num_examples\n",
    "val_len = builder.info.splits[\"test\"].num_examples\n",
    "steps_per_epoch = train_len//batch_size\n",
    "val_steps = val_len//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_batched = train_ds.batch(batch_size, drop_remainder=True)\n",
    "test_ds_batched = test_ds.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**\n",
    "\n",
    "Now it is time to train the $\\beta$-VAE.\n",
    "\n",
    "First, create a CVAE model with 2 latent dimensions. Then, create an Adam optimizer with a learning rate of 0.001. Finally, implement the iterative training procedure.\n",
    "\n",
    "Training pseudocode:\n",
    "```\n",
    "for every epoch\n",
    "    for every batch in train dataset\n",
    "        compute loss and apply gradients\n",
    "        \n",
    "    for every batch in test dataset\n",
    "        compute loss\n",
    "    \n",
    "    print test set loss\n",
    "```\n",
    "Make sure to use the `compute_apply_gradients` and `compute_loss` functions. Set $\\beta$ to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The latent representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section we have a look at the latent representation for the digits in the test set. \n",
    "\n",
    "To do this, we simply feed images from the test set to the CVAE and record the $\\mu$ and $\\sigma$ for each one. We can then sample from the multivariate normal $\\mathcal{N}(\\mu, \\sigma)$ to get a representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**\n",
    "\n",
    "Complete the code below to inspect the latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_means = np.empty((val_steps*batch_size, latent_dim), dtype=np.float32)\n",
    "z_logvars = np.empty((val_steps*batch_size, latent_dim), dtype=np.float32)\n",
    "z = np.empty((val_steps*batch_size, latent_dim))\n",
    "labels = []\n",
    "\n",
    "for i, (batch, l) in enumerate(test_ds_batched):\n",
    "    m, lv = ... # use the model to get the multivariate normal paramters\n",
    "    z_means[i*batch_size:(i+1)*batch_size] = m \n",
    "    z_logvars[i*batch_size:(i+1)*batch_size] = lv\n",
    "    \n",
    "    z[i*batch_size:(i+1)*batch_size] = ... # use the model to sample a latent vector\n",
    "    \n",
    "    labels.extend(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot a histogram for the mean and standard deviations returned by the model. What do you expect to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(latent_dim):\n",
    "    plt.hist(z_means[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(latent_dim):\n",
    "    plt.hist(np.sqrt(np.exp(z_logvars[:, i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the prior and posterior distributions over the latent representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "x, y = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "pos = np.dstack((x, y))\n",
    "rv = multivariate_normal([0, 0], [[1, 0], [0, 1]])\n",
    "plt.contourf(x, y, rv.pdf(pos))\n",
    "plt.title(\"Normal prior p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the average multivariate normal per digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(np.unique(labels)), figsize=(20, 2), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    rv = multivariate_normal(np.mean(z_means[np.nonzero(labels==i)], axis=0), np.eye(2)*np.mean(np.sqrt(np.exp(z_logvars[np.nonzero(labels==i)])), axis=0))\n",
    "    ax.contourf(x, y, rv.pdf(pos))\n",
    "    ax.set_title(\"q(z|x, y=%d)\" % i)\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot the latent representation sampled from the normal latent posterior for every test instance, colored according to the ground truth label. Can you see the link between the plots above and the one below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array(list(map(lambda l: cm.tab10(l), labels)))\n",
    "\n",
    "for i in np.unique(labels):\n",
    "    plt.scatter(z[np.nonzero(labels==i), 0], z[np.nonzero(labels==i), 1], s=0.5, c=colors[np.nonzero(labels==i)], label=i)\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.legend(markerscale=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final section we plot an *interpolation grid* by sampling from the data generating distribution $p_\\theta(x|z)$ at points defined by the normal prior. \n",
    "\n",
    "Note that we are able to choose meaningful latent embeddings, since we know that they are supposed to be distributed according to a unit normal.\n",
    "\n",
    "Again, have a look at the interpolation grid and make the link with the multivariate normals per digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_images(model, n, digit_size=28):\n",
    "    \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
    "\n",
    "    norm = tfp.distributions.Normal(0, 1)\n",
    "    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    image_width = digit_size*n\n",
    "    image_height = image_width\n",
    "    image = np.zeros((image_height, image_width))\n",
    "\n",
    "    for i, yi in enumerate(grid_y[::-1]):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z = np.array([[xi, yi]])\n",
    "            x_decoded = model.sample(z)\n",
    "            digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
    "            image[i * digit_size: (i + 1) * digit_size,\n",
    "                j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap='Greys_r')\n",
    "    plt.axis('Off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_images(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
