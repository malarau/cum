{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a12340",
   "metadata": {},
   "source": [
    "# Practical session 1: Regression and Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c37e15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Table of Contents <a class=\"anchor\" id=\"top\"></a>\n",
    "* [1. Using regression and classification models](#section_1)\n",
    "    * [1.1 Linear regression](#section_1_1)\n",
    "    * [1.2 K-nearest neighbors regression](#section_1_2)\n",
    "    * [1.3 Logistic regression](#section_1_3)\n",
    "* [2. Implementing resampling methods](#section_2)\n",
    "    * [2.1 Linear regression](#section_2_1)\n",
    "    * [2.2. KNN regression](#section_2_2)\n",
    "    * [2.3. Logistic regression](#section_2_3)\n",
    "* [3. Dimensionality reduction](#section_3)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bb65b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p>Green blocks indicate sections where you have to implement code, usually by replacing the ellipses [...].</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab751e21",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Yellow blocks are optional questions and/or exercises.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e68e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7616f",
   "metadata": {},
   "source": [
    "By now, we have seen two regression methods for predicting continuous output (linear regression and K-nearest neighbors), and one classification method for predicting discrete output (logistic regression). In addition, we have learned about model assessment through different resampling methods, i.e., train-test validation, (leave-one-out) cross-validation, and bootstrapping.\n",
    "\n",
    "In this practical session, we will implement these predictive models and resampling methods using the **scikit-learn** library (written as `sklearn` when you import it), which we have already installed when creating the conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7bcf3",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> 1. Using regression and classification models </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2409ca",
   "metadata": {},
   "source": [
    "Scikit-learn is a collection of tools for predictive data anlysis. Among others, it has a collection of [supervised learning models](https://scikit-learn.org/stable/supervised_learning.html), with extensive [documentation](https://scikit-learn.org/stable/modules/classes.html) on each model which also contains example code. The models we are using in this practical are\n",
    "* [1.1.1 Ordinary Least Squares](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) ([API](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)), \n",
    "* [1.6.3 Nearest Neighbors Regression](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression) ([API](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)), and\n",
    "* [1.1.11 Logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) ([API](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)).\n",
    "\n",
    "\n",
    "For now, we will follow these steps when building a regression/classification model:\n",
    "1. Load the dataset.\n",
    "2. Declare the model we will use.\n",
    "3. Fit the training data to the model.\n",
    "4. Use the model to predict unseen data.\n",
    "\n",
    "Fortunately, scikit-learn already has a few built-in [datasets](https://scikit-learn.org/stable/datasets.html). In this practical we will only be using the smaller datasets, or [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html), namely the diabetes dataset for regression, and iris dataset for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6661d",
   "metadata": {},
   "source": [
    "<h3 style=\"display: inline\"> 1.1 Linear regression </h3> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1_1\"></a> \n",
    "\n",
    "In this section, we demonstrate how to apply linear regression on the diabetes dataset. This dataset contains 442 instances and 10 features, e.g., age, sex, and bmi, to predict a quantitative measure of diabetes progression one year after baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c56fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the dataset\n",
    "from sklearn import datasets\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y = True, as_frame = True, scaled = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d36d1",
   "metadata": {},
   "source": [
    "You can check what the arguments of the `load_diabetes` function does [here](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes). Essentially,\n",
    "* `return_X_y = True` tells the function to return a separate object for the predictors (X) and the response variable (y),\n",
    "* `as_frame = True` tells the function to return a `pandas.DataFrame` object, and\n",
    "* `scaled = False` returns unscaled values of the data.\n",
    "\n",
    "[`pandas`](https://pandas.pydata.org/docs/user_guide/index.html) is a library that helps you work with datasets by \n",
    "providing functions for analyzing, cleaning, exploring, and manipulating data. For example, with the command `head`, you can see the first few rows of your data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors (or features, or independent variables)\n",
    "diabetes_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response (or dependent variable)\n",
    "diabetes_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3f483",
   "metadata": {},
   "source": [
    "For the sake of visualization, let us make a model with only 1 predictor, the bmi. You can easily subset a `DataFrame` object by putting its column name(s) in square brackets. However, a single bracket will output a `pandas.Series` object (which we can't directly use to fit the model), whereas a double bracket will output a `pandas.DataFrame` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6818b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Subsetting with single brackets\")\n",
    "print(\"Data type: {}, \\nDimension: {}\\n\".format(type(diabetes_X['bmi']), diabetes_X['bmi'].shape))\n",
    "\n",
    "print(\"Subsetting with double brackets\")\n",
    "print(\"Data type: {}, \\nDimension: {}\".format(type(diabetes_X[['bmi']]), diabetes_X[['bmi']].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8603d43",
   "metadata": {},
   "source": [
    "Now, let's build the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265792ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 2. Declare the model\n",
    "linreg_model = LinearRegression()\n",
    "\n",
    "# 3. Fit the model to the dataset\n",
    "linreg_model.fit(diabetes_X[['bmi']], diabetes_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839e8bf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Fit the model on the data subsetted with single brackets. Try to fix the error by following the instructions on the error message.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a098f5c",
   "metadata": {},
   "source": [
    "Before we go on to predicting new output, let's first visualize what our model is doing. Recall that linear regression is just finding a straight line that best fits the data. This line can be drawn by getting the intercept and $\\beta_1$ cofficient from the model through the the `intercept_` and `coef_` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linreg_model.intercept_)\n",
    "print(linreg_model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925dd197",
   "metadata": {},
   "source": [
    "We can overlay this line over the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two points: the minimum and maximum of the bmi\n",
    "x1 = diabetes_X[['bmi']].min()\n",
    "x2 = diabetes_X[['bmi']].max()\n",
    "\n",
    "# Calculate y values using slope-intercept equation\n",
    "y1 = linreg_model.coef_[0] * x1 + linreg_model.intercept_\n",
    "y2 = linreg_model.coef_[0] * x2 + linreg_model.intercept_\n",
    "\n",
    "plt.scatter(diabetes_X[['bmi']], diabetes_y)   # Plot real data\n",
    "plt.plot([x1, x2], [y1, y2], color='red')      # Plot predicted line\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Disease progression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c0a8b",
   "metadata": {},
   "source": [
    "Finally, you can use the model to predict what the expected disease progression is for a given BMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use the model to predict new output\n",
    "# e.g., people with BMI of 25, 35, and 28.5\n",
    "linreg_model.predict([[25], [35], [28.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a776f4",
   "metadata": {},
   "source": [
    "Note that these values lie on the red line of the figure above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950cefa1",
   "metadata": {},
   "source": [
    "<h3 style=\"display: inline\"> 1.2 K-nearest neighbors regression </h3> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1_2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c7c6e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "        <p>Follow the same workflow as above and perform KNN regression on the diabetes dataset, using only the BMI as the feature.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db95f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset already loaded (diabetes_X[['bmi']] and diabetes_y)\n",
    "\n",
    "# Import k-nearest neighbors model from sklearn\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 2. Declare the model\n",
    "knn_model = ...\n",
    "\n",
    "# 3. Fit the model to the dataset\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcadaf",
   "metadata": {},
   "source": [
    "Since KNN is not a linear model, it has no `coef_` or `intercept_` attribute, and we cannot draw a straight line to see where our predictions will lie on. We will visualize the predicted values by creating the entire range of x values and seeing what is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 evenly-spaced data points between the minimum and maximum bmi values\n",
    "x_values = np.linspace(diabetes_X[['bmi']].min(), diabetes_X[['bmi']].max(), num = 100)\n",
    "\n",
    "# 4. Predict\n",
    "y_values = knn_model.predict(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(diabetes_X[['bmi']], diabetes_y)   # Plot real data\n",
    "plt.plot(x_values, y_values, color='red')      # Plot predicted values\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Disease progression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e4ce98",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\"> 1.3 Logistic regression </h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_1_3\"></a>\n",
    "\n",
    "For this, we will use the iris dataset to perform classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ee2bb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    <p>Load and <b>explore</b> the iris toy dataset, then follow the same workflow as above to perform logistic regression.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the iris dataset\n",
    "iris_X, iris_y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f680c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    How many features are in this dataset? How many samples? How many classes of irises are there?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} samples x {} features\".format(iris_X.shape[0], iris_X.shape[1]))\n",
    "print(len(iris_X))     # this also returns the number of rows (samples)\n",
    "print(\"Classes:\", set(iris_y))     # easy way to see unique values - 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d756be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_y.value_counts()) # Easy way to show how many instances are in a class\n",
    "# Results mean that there are 50 instances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcc3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import logistic regression model from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 2. Declare the model\n",
    "logreg_model = ...\n",
    "\n",
    "# 3. Fit the model to the dataset\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa57a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    If you got a <b>ConvergenceWarning</b>, try reading the error message and see if you can fix it!<br> <b>Solution:</b> include <code>max_iter</code> in the model declaration, so <code>LogisticRegression(max_iter=1000)</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea250f",
   "metadata": {},
   "source": [
    "Try predicting the class of the first 10 samples of the dataset, are these the same as the true classes in `iris_y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_iris_y = ...\n",
    "\n",
    "# Check if predicted_iris_y is the same as the iris_y\n",
    "all(predicted_iris_y == iris_y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215ec42",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\">2. Implementing resampling methods</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f7e73",
   "metadata": {},
   "source": [
    "In the previous section, we trained our models on the entire dataset, which can lead to overfitting of the data and underestimation of the error. We will mitigate this by using different resampling methods and evaluating model performance. This adds two more steps from our approach above:\n",
    "\n",
    "<ol>\n",
    "<li> Load the dataset. </li>\n",
    "<b><li>Partition the dataset.</li></b>\n",
    "<li>Declare the model we will use.</li>\n",
    "<li>Fit the training data to the model.</li>\n",
    "<li>Use he model to predict unseen data.</li>\n",
    "<b><li>Evaluate the model.</li></b>\n",
    "</ol>\n",
    "\n",
    "These new steps go hand-in-hand and are also grouped together in [Chapter 3](https://scikit-learn.org/stable/model_selection.html) of the scikit-learn user guide, with documentation of the resampling methods found [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection).\n",
    "\n",
    "Once again, we will start with the linear regression model on the diabetes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefefc7",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\">2.1 Linear regression</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2_1\"></a>\n",
    "\n",
    "#### 2.1.1 Evaluate model performance with no partitioning\n",
    "\n",
    "We will repeat the code from section 1.1 here with some changes. Mainly, we demonstrate the use of the `score` function, which is a general function for evaluating models. For linear regression, it computes the $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set scaled = True so features are more comparable\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y = True, as_frame = True, scaled = True)\n",
    "\n",
    "# Can also combine two steps like this\n",
    "# We will use the entire dataset this time\n",
    "linreg_model = LinearRegression().fit(diabetes_X, diabetes_y)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "linreg_model.score(diabetes_X, diabetes_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2b69b",
   "metadata": {},
   "source": [
    "#### 2.1.2 Evaluate model performance with train-test partitioning\n",
    "\n",
    "Partitioning of the dataset can be done with [`model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) (default: 0.75-0.25 split). The `random_state` parameter is used for reproducibility, so everyone who runs the code with the same random state has their data split the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332452f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    <p>Partition the data with <code>train_test_split</code> and compare model performance on training and test data. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "db_X_train, db_X_test, db_y_train, db_y_test = ...\n",
    "\n",
    "# Fit data only on training model\n",
    "linreg_model = ...\n",
    "\n",
    "linreg_train_score = linreg_model.score(db_X_train, db_y_train)\n",
    "linreg_test_score = ...\n",
    "\n",
    "# Compare model performance on test vs train data\n",
    "print(\"Score on train data:\", linreg_train_score)\n",
    "print(\"Score on test data:\", linreg_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619bfa47",
   "metadata": {},
   "source": [
    "As expected, our model performs worse on data it has never seen before (the test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f29e9",
   "metadata": {},
   "source": [
    "#### 2.1.3 Cross-validation\n",
    "\n",
    "[`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) is an object class that is used to generate groups for cross-validation. It generates indices for splitting the data into `n_split` number of groups. We visualize this below (you can ignore the code inside the `for` loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a69c6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "fig, ax = plt.subplots() # Create empty figure\n",
    "\n",
    "for fold_i, (train_index, test_index) in enumerate(kf.split(diabetes_X, diabetes_y)):\n",
    "    \n",
    "    n_samples = len(diabetes_X)\n",
    "    \n",
    "    # Create an array of zeros 1 and make the test set indices = 1\n",
    "    indices = np.zeros(n_samples)\n",
    "    indices[test_index] = 1\n",
    "\n",
    "    # Visualize the results\n",
    "    ax.scatter(range(n_samples),\n",
    "               [fold_i + 1] * n_samples,  \n",
    "               c=indices,              # Color is determined values of the array (0 or 1)\n",
    "               marker=\"_\", lw=10,\n",
    "               vmin=-0.2, vmax=1.2)\n",
    "\n",
    "# Formatting\n",
    "ax.set(xlabel=\"Sample index\", ylabel=\"CV iteration\",\n",
    "       yticks=np.arange(n_splits) + 1, ylim=[n_splits + 1, -0.2])\n",
    "\n",
    "ax.legend([\"Testing set\", \"Training set\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f5c35",
   "metadata": {},
   "source": [
    "As you can see, the dataset gets partitioned into 5 groups, with the test set changing every fold. What is important from the code block above is that you will have to **iterate the `KFold` object you declared in a `for` loop.** You can then train and evaluate the model inside each loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736c22b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    <p>Implement 5-fold cross validation by filling in the code below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_fold_train = []\n",
    "scores_per_fold_test = []\n",
    "\n",
    "n_splits = 5\n",
    "kf = ...\n",
    "\n",
    "i = 1\n",
    "for (train_index, test_index) in ...:\n",
    "    # Subset training indices\n",
    "    db_X_train_i = diabetes_X.iloc[...]\n",
    "    db_y_train_i = diabetes_y.iloc[...]\n",
    "    \n",
    "    # Subset test indices\n",
    "    db_X_test_i = diabetes_X.iloc[...]\n",
    "    db_y_test_i = diabetes_y.iloc[...]\n",
    "    \n",
    "    # Declare model and fit data only on training model of this fold\n",
    "    linreg_model = ...\n",
    "\n",
    "    # Store model performance on test vs train data\n",
    "    scores_per_fold_train.append(...)\n",
    "    scores_per_fold_test.append(...)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "# Just code for printing, can ignore\n",
    "printing = list(zip(np.arange(1, n_splits + 1), scores_per_fold_train, scores_per_fold_test))\n",
    "print('\\n'.join('Fold {}\\n  Train: {:.2f}\\n  Test: {:.2f}'.format(*fold) for fold in printing))\n",
    "print('Average train score: {:.3f}'.format(np.mean(scores_per_fold_train)))\n",
    "print('Average test score: {:.3f}'.format(np.mean(scores_per_fold_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca762c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Leave-one-out cross-validation (LOOCV) is a special case of cross-validation. What would <b>n_splits</b> have to be if you want to perform LOOCV?<br>\n",
    "    <b>Answer:</b> It would be the equal to the number of samples, so 442 in this case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b92413",
   "metadata": {},
   "source": [
    "<h3 style=\"display: inline\">2.2 KNN  regression</h3> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2_2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675e085",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "        <p>Implement KNN on the train-test partitioning and 5-fold cross-validation. In both cases, compare scores between training and test data.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5741d86e",
   "metadata": {},
   "source": [
    "#### 2.2.1 Train-test partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y = True, as_frame = True, scaled = True)\n",
    "\n",
    "# Partition data into train and test data\n",
    "db_X_train, db_X_test, db_y_train, db_y_test = train_test_split(diabetes_X, diabetes_y)\n",
    "\n",
    "# Declare model and fit\n",
    "knn_model = KNeighborsRegressor().fit(db_X_train, db_y_train)\n",
    "\n",
    "# Compare scores\n",
    "knn_train_score = knn_model.score(db_X_train, db_y_train)\n",
    "knn_test_score = knn_model.score(db_X_test, db_y_test)\n",
    "\n",
    "print(\"Score on train data:\", knn_train_score)\n",
    "print(\"Score on test data:\", knn_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e8de8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    1. Compared to linear regression, the difference between the train and test performance of KNN is much higher. Can you explain this? <br> 2. What is the default number of neighbors used by the model? <br>   \n",
    "    3. Try using different number of neighbors for the model and compare their performances.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d2d83",
   "metadata": {},
   "source": [
    "#### 2.2.2 Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c2316",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Challenge:</b> Try and see if you can implement this without looking at the code from the previous section! (But looking at the documentation is allowed.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e7fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_per_fold_train = []\n",
    "scores_per_fold_test = []\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "i = 1\n",
    "for (train_index, test_index) in kf.split(diabetes_X, diabetes_y):\n",
    "    db_X_train_i = diabetes_X.iloc[train_index]\n",
    "    db_y_train_i = diabetes_y.iloc[train_index]\n",
    "    \n",
    "    db_X_test_i = diabetes_X.iloc[test_index]\n",
    "    db_y_test_i = diabetes_y.iloc[test_index]\n",
    "    \n",
    "    # Fit data only on training model of this fold\n",
    "    knn_model = KNeighborsRegressor().fit(db_X_train_i, db_y_train_i)\n",
    "\n",
    "    # Store model performance on test vs train data\n",
    "    scores_per_fold_train.append(knn_model.score(db_X_train_i, db_y_train_i))\n",
    "    scores_per_fold_test.append(knn_model.score(db_X_test_i, db_y_test_i))\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print('Average train score: {:.3f}'.format(np.mean(scores_per_fold_train)))\n",
    "print('Average test score: {:.3f}'.format(np.mean(scores_per_fold_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abdff7",
   "metadata": {},
   "source": [
    "<h3 style=\"display: inline\">2.3 Logistic regression</h3> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_2_3\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b57d8a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "        <p>Implement logistic regression on the train-test partitioning and 5-fold cross-validation of the iris dataset. In both cases, compare scores between training and test data.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a33ae",
   "metadata": {},
   "source": [
    "#### 2.3.1 Train-test partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "...\n",
    "\n",
    "# Partition data into train and test data\n",
    "...\n",
    "\n",
    "# Declare model and fit\n",
    "logreg_model = ...\n",
    "\n",
    "# Compare scores\n",
    "logreg_train_score = ...\n",
    "logreg_test_score = ...\n",
    "\n",
    "print(\"Score on train data:\", logreg_train_score)\n",
    "print(\"Score on test data:\", logreg_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089121b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    For regression models, the score is the $R^2$ value. What does the <b>score</b> refer to for logistic regression? <br> <b>Answer:</b> The accuracy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d4069",
   "metadata": {},
   "source": [
    "In this case, the accuracy is the fraction of correct predictions. We can calculate this ourselves or use the [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) function. We see that both cases return the same value as the `score` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate this ourselves\n",
    "print(np.sum(logreg_model.predict(iris_X_test) == iris_y_test)/len(iris_y_test))  \n",
    "\n",
    "# or use premade function\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(logreg_model.predict(iris_X_test), iris_y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b59bc",
   "metadata": {},
   "source": [
    "#### 2.3.2 Cross-validation\n",
    "\n",
    "For this section, it is important that you use `StratifiedKFold` instead of `KFold`. Recall the barplot we generated in 2.1.3, and notice that `KFold` splits the dataset into groups according in an orderly manner. Now notice how the `iris` dataset is organized (the different colors refer to different classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f29b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 1)) # Create empty figure\n",
    "n_samples = len(iris_X)\n",
    "    \n",
    "ax.scatter(range(n_samples), [1]*n_samples, c=iris_y, marker=\"_\", lw=10, cmap = plt.cm.Paired)\n",
    "    \n",
    "# Formatting\n",
    "ax.set(xlabel=\"Sample index\")\n",
    "ax.yaxis.set_tick_params(labelleft=False)\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5cbd8",
   "metadata": {},
   "source": [
    "Unlike `Kfold`, each group created by `StratifiedKFold` will have an equal number of samples from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ...\n",
    "\n",
    "scores_per_fold_train = []\n",
    "scores_per_fold_test = []\n",
    "\n",
    "kf = ...\n",
    "\n",
    "i = 1\n",
    "for ... in ...:\n",
    "    \n",
    "    # Training data\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    # Test data\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    # Fit data only on training model of this fold\n",
    "    logreg_model = ...  \n",
    "\n",
    "    # Store model performance on test vs train data\n",
    "    scores_per_fold_train.append(...)\n",
    "    scores_per_fold_test.append(...)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print('Average train score: {:.3f}'.format(np.mean(scores_per_fold_train)))\n",
    "print('Average test score: {:.3f}'.format(np.mean(scores_per_fold_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c397b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Compare the results of using <code>KFold</code> with <code>StratifiedKFold</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cea090",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Implement bootstrapping for KNN and logistic regression. Do the training and test scores follow the same distribution as those of linear regression? <br> Additionally, compare the runtime of all three methods over these bootstrapping iterations. Does this match with their computational complexity?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852bbc16",
   "metadata": {},
   "source": [
    "<h2 style=\"display: inline\">3. Dimensionality reduction</h2> <span style=\"float: right\"><a href=\"#top\">[back to top]</a></span> <a class=\"anchor\" id=\"section_3\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdab78",
   "metadata": {},
   "source": [
    "Now it's time to include some extra preprocessing: dimensionality reduction. The Iris dataset has only 4 features so it is not very interesting to test this, but diabetes has 10, so we can try to find a smaller set of features that give the same (or better) prediccion error.\n",
    "\n",
    "We have to be careful to NEVER use the validation/test set to train the dimensionality reduction, so in a k-fold cross validation, we have to re-train the dimensionality reduction for every fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a52fe",
   "metadata": {},
   "source": [
    "Dimensionality reduction is an important tool in data science, it performs a transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data. These properties vary with each algorithm.\n",
    "\n",
    "**Principal component analysis (PCA)** might be the most known dimensionality reduction method. It's goal is to project the data to a lower dimension coordinate system by preserving as much variance in the data as possible. The (new) axes of the lower dimensional data are then called principal components (PCs)\n",
    "\n",
    "**Linear Discriminant analysis** (introduced during week 3 as a classifier) can also be used as a dimensionality reduction method, but its objective for dimension reduction is not maximizing variance, but finding the lower dimensional space that best separates the classes present in the data.\n",
    "\n",
    "![CV](./PCA_LDA.png)\n",
    "\n",
    "Many other exist, such as **UMAP**,  **t-SNE**, **MDS** etc.\n",
    "\n",
    "Dimensionality reduction is used within data science for two purposes:\n",
    "<ol>\n",
    "    <li> <i>Data visualisation</i>\n",
    "    <li> <i>Feature reduction</i>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a0f81",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3> Exercise:</h3>\n",
    "<ol>\n",
    "    <li> Perform <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\">5-fold CV</a>\n",
    "    <li> Scale the data \n",
    "    <li> Perform a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">PCA</a> transformation and make sure 85% of the variation is retained in the lower dimension\n",
    "    <li> Perform a regression with a Linear Regressor</a>\n",
    "    <li> Evaluate the model with the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error\">mean squared error</a>\n",
    "    <li> Plot the first PCs against each other colored by class label (<b>HINT</b>: you can access the fitted objects in the pipeline)\n",
    "<ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Declare the K-fold constructor\n",
    "...\n",
    "\n",
    "# initialize an array to store the rmse per fold\n",
    "scores = np.zeros(5)\n",
    "\n",
    "# Perform 5-fold CV\n",
    "for ...:\n",
    "    # Per split, construct the training and test data\n",
    "    ...\n",
    "    \n",
    "    # Declare the pipeline constructor\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('dimensionality_reduction', PCA(n_components=0.85)),\n",
    "        ('regression', LinearRegression()),\n",
    "    ]) \n",
    "    \n",
    "    # Fit and evaluate the pipeline\n",
    "    ...\n",
    "\n",
    "    # Measure and store the accuracy per fold\n",
    "    scores[i] = ...\n",
    "\n",
    "\n",
    "print('mean MSE', np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d1ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this part we will use the model constructed in the last fold that is still present in working environment of the notebook\n",
    "# Scale the training data\n",
    "X_train_scaled = pipe['scaler'].transform(X_train)\n",
    "\n",
    "# Extract the first three dimensions of the transformed training data\n",
    "pc_0 = pipe['dimensionality_reduction'].transform(X_train_scaled)[:,0]\n",
    "pc_1 = pipe['dimensionality_reduction'].transform(X_train_scaled)[:,1]\n",
    "pc_2 = pipe['dimensionality_reduction'].transform(X_train_scaled)[:,2]\n",
    "\n",
    "# Scale the test data\n",
    "X_test_scaled = pipe['scaler'].transform(X_test)\n",
    "\n",
    "# Extract the first three dimensions of the transformed test data\n",
    "pc_0_test = pipe['dimensionality_reduction'].transform(X_test_scaled)[:,0]\n",
    "pc_1_test = pipe['dimensionality_reduction'].transform(X_test_scaled)[:,1]\n",
    "pc_2_test = pipe['dimensionality_reduction'].transform(X_test_scaled)[:,2]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,5))\n",
    "ax[0].scatter(pc_0, pc_1, c=y_train)\n",
    "ax[0].set_title('PC0-PC1 Train')\n",
    "ax[0].set_xlabel('PC 0')\n",
    "ax[0].set_ylabel('PC 1')\n",
    "\n",
    "ax[1].scatter(pc_0_test, pc_1_test, c=y_test)\n",
    "ax[1].set_title('PC0-PC1 Test')\n",
    "ax[1].set_xlabel('PC 0')\n",
    "ax[1].set_ylabel('PC 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364dd62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
